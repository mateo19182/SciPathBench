<html>
    <head>
        <meta charset="utf-8">
        
            <script src="lib/bindings/utils.js"></script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 100%;
                 height: 750px;
                 background-color: #ffffff;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             

             

             
             /* position absolute is important and the container has to be relative or absolute as well. */
          div.popup {
                 position:absolute;
                 top:0px;
                 left:0px;
                 display:none;
                 background-color:#f5f4ed;
                 -moz-border-radius: 3px;
                 -webkit-border-radius: 3px;
                 border-radius: 3px;
                 border: 1px solid #808074;
                 box-shadow: 3px 3px 10px rgba(0, 0, 0, 0.2);
          }

          /* hide the original tooltip */
          .vis-tooltip {
            display:none;
          }
             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"color": "#999999", "font": {"color": "#000000"}, "id": "W2097356275", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eHigh-Performance Neural Networks for Visual Object Classification\u003c/b\u003e\u003cbr\u003eID: W2097356275\u003cbr\u003eYear: 2011\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.48550/arxiv.1102.0183\u0027\u003ehttps://doi.org/10.48550/arxiv.1102.0183\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2114153178", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eRate-coded Restricted Boltzmann Machines for Face Recognition\u003c/b\u003e\u003cbr\u003eID: W2114153178\u003cbr\u003eYear: 2000\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027N/A\u0027\u003eN/A\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2100495367", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eReducing the Dimensionality of Data with Neural Networks\u003c/b\u003e\u003cbr\u003eID: W2100495367\u003cbr\u003eYear: 2006\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1126/science.1127647\u0027\u003ehttps://doi.org/10.1126/science.1127647\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2064082346", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003ePhone recognition using Restricted Boltzmann Machines\u003c/b\u003e\u003cbr\u003eID: W2064082346\u003cbr\u003eYear: 2010\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1109/icassp.2010.5495651\u0027\u003ehttps://doi.org/10.1109/icassp.2010.5495651\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2115761837", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003ePermitted and Forbidden Sets in Symmetric Threshold-Linear Networks\u003c/b\u003e\u003cbr\u003eID: W2115761837\u003cbr\u003eYear: 2003\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1162/089976603321192103\u0027\u003ehttps://doi.org/10.1162/089976603321192103\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2108598243", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eImageNet: A large-scale hierarchical image database\u003c/b\u003e\u003cbr\u003eID: W2108598243\u003cbr\u003eYear: 2009\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1109/cvpr.2009.5206848\u0027\u003ehttps://doi.org/10.1109/cvpr.2009.5206848\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2015861736", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eConvolutional networks and applications in vision\u003c/b\u003e\u003cbr\u003eID: W2015861736\u003cbr\u003eYear: 2010\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1109/iscas.2010.5537907\u0027\u003ehttps://doi.org/10.1109/iscas.2010.5537907\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2018435387", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eTaylor expansion of the accumulated rounding error\u003c/b\u003e\u003cbr\u003eID: W2018435387\u003cbr\u003eYear: 1976\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1007/bf01931367\u0027\u003ehttps://doi.org/10.1007/bf01931367\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2157364932", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eLearning a Similarity Metric Discriminatively, with Application to Face Verification\u003c/b\u003e\u003cbr\u003eID: W2157364932\u003cbr\u003eYear: 2005\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1109/cvpr.2005.202\u0027\u003ehttps://doi.org/10.1109/cvpr.2005.202\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2166049352", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eLearning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories\u003c/b\u003e\u003cbr\u003eID: W2166049352\u003cbr\u003eYear: 2007\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1016/j.cviu.2005.09.012\u0027\u003ehttps://doi.org/10.1016/j.cviu.2005.09.012\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2134557905", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eLearning methods for generic object recognition with invariance to pose and lighting\u003c/b\u003e\u003cbr\u003eID: W2134557905\u003cbr\u003eYear: 2004\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1109/cvpr.2004.1315150\u0027\u003ehttps://doi.org/10.1109/cvpr.2004.1315150\u003c/a\u003e"}, {"color": "#00ff00", "font": {"color": "#000000"}, "id": "W2618530766", "label": "ImageNet classification with deep convolutional neural networks [Start]", "shape": "dot", "size": 35, "title": "\u003cb\u003eImageNet classification with deep convolutional neural networks\u003c/b\u003e\u003cbr\u003eID: W2618530766\u003cbr\u003eYear: 2017\u003cbr\u003ePath: both\u003cbr\u003eType: start\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1145/3065386\u0027\u003ehttps://doi.org/10.1145/3065386\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2101926813", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eNeocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position\u003c/b\u003e\u003cbr\u003eID: W2101926813\u003cbr\u003eYear: 1980\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1007/bf00344251\u0027\u003ehttps://doi.org/10.1007/bf00344251\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2141125852", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eMulti-column deep neural networks for image classification\u003c/b\u003e\u003cbr\u003eID: W2141125852\u003cbr\u003eYear: 2012\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1109/cvpr.2012.6248110\u0027\u003ehttps://doi.org/10.1109/cvpr.2012.6248110\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2061212083", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eLessons from the Netflix prize challenge\u003c/b\u003e\u003cbr\u003eID: W2061212083\u003cbr\u003eYear: 2007\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1145/1345448.1345465\u0027\u003ehttps://doi.org/10.1145/1345448.1345465\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2156163116", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eBest practices for convolutional neural networks applied to visual document analysis\u003c/b\u003e\u003cbr\u003eID: W2156163116\u003cbr\u003eYear: 2005\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1109/icdar.2003.1227801\u0027\u003ehttps://doi.org/10.1109/icdar.2003.1227801\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2097117768", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eGoing deeper with convolutions\u003c/b\u003e\u003cbr\u003eID: W2097117768\u003cbr\u003eYear: 2015\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1109/cvpr.2015.7298594\u0027\u003ehttps://doi.org/10.1109/cvpr.2015.7298594\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2149845449", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eImplicit Mixtures of Restricted Boltzmann Machines\u003c/b\u003e\u003cbr\u003eID: W2149845449\u003cbr\u003eYear: 2008\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027N/A\u0027\u003eN/A\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2169805405", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eConvolutional Networks Can Learn to Generate Affinity Graphs for Image Segmentation\u003c/b\u003e\u003cbr\u003eID: W2169805405\u003cbr\u003eYear: 2009\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1162/neco.2009.10-08-881\u0027\u003ehttps://doi.org/10.1162/neco.2009.10-08-881\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W1904365287", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eImproving neural networks by preventing co-adaptation of feature detectors\u003c/b\u003e\u003cbr\u003eID: W1904365287\u003cbr\u003eYear: 2012\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.48550/arxiv.1207.0580\u0027\u003ehttps://doi.org/10.48550/arxiv.1207.0580\u003c/a\u003e"}, {"color": "#1f77b4", "font": {"color": "#000000"}, "id": "W1545791100", "label": "Paper W1545791100", "shape": "dot", "size": 25, "title": "\u003cb\u003ePaper W1545791100\u003c/b\u003e\u003cbr\u003eID: W1545791100\u003cbr\u003eYear: Unknown\u003cbr\u003ePath: ground_truth\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027N/A\u0027\u003eN/A\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W145818128", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eDiffusion Networks, Products of Experts, and Factor Analysis\u003c/b\u003e\u003cbr\u003eID: W145818128\u003cbr\u003eYear: 2001\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027N/A\u0027\u003eN/A\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2136922672", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eA Fast Learning Algorithm for Deep Belief Nets\u003c/b\u003e\u003cbr\u003eID: W2136922672\u003cbr\u003eYear: 2006\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1162/neco.2006.18.7.1527\u0027\u003ehttps://doi.org/10.1162/neco.2006.18.7.1527\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2108069432", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eMulti-column deep neural network for traffic sign classification\u003c/b\u003e\u003cbr\u003eID: W2108069432\u003cbr\u003eYear: 2012\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1016/j.neunet.2012.02.023\u0027\u003ehttps://doi.org/10.1016/j.neunet.2012.02.023\u003c/a\u003e"}, {"color": "#1f77b4", "font": {"color": "#000000"}, "id": "W1995875735", "label": "A Mathematical Theory of Communication [Target]", "shape": "dot", "size": 35, "title": "\u003cb\u003eA Mathematical Theory of Communication\u003c/b\u003e\u003cbr\u003eID: W1995875735\u003cbr\u003eYear: 1948\u003cbr\u003ePath: ground_truth\u003cbr\u003eType: end\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1002/j.1538-7305.1948.tb01338.x\u0027\u003ehttps://doi.org/10.1002/j.1538-7305.1948.tb01338.x\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W1782590233", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eLabeled Faces in the Wild: A Database forStudying Face Recognition in Unconstrained Environments\u003c/b\u003e\u003cbr\u003eID: W1782590233\u003cbr\u003eYear: 2008\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027N/A\u0027\u003eN/A\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2154579312", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eHandwritten Digit Recognition with a Back-Propagation Network\u003c/b\u003e\u003cbr\u003eID: W2154579312\u003cbr\u003eYear: 1989\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027N/A\u0027\u003eN/A\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2110764733", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eLabelMe: A Database and Web-Based Tool for Image Annotation\u003c/b\u003e\u003cbr\u003eID: W2110764733\u003cbr\u003eYear: 2007\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1007/s11263-007-0090-8\u0027\u003ehttps://doi.org/10.1007/s11263-007-0090-8\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2144161366", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eA High-Throughput Screening Approach to Discovering Good Forms of Biologically Inspired Visual Representation\u003c/b\u003e\u003cbr\u003eID: W2144161366\u003cbr\u003eYear: 2009\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1371/journal.pcbi.1000579\u0027\u003ehttps://doi.org/10.1371/journal.pcbi.1000579\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2053229256", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eHigh-dimensional signature compression for large-scale image classification\u003c/b\u003e\u003cbr\u003eID: W2053229256\u003cbr\u003eYear: 2011\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1109/cvpr.2011.5995504\u0027\u003ehttps://doi.org/10.1109/cvpr.2011.5995504\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2130325614", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eConvolutional deep belief networks for scalable unsupervised learning of hierarchical representations\u003c/b\u003e\u003cbr\u003eID: W2130325614\u003cbr\u003eYear: 2009\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1145/1553374.1553453\u0027\u003ehttps://doi.org/10.1145/1553374.1553453\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2158164339", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eModeling Human Motion Using Binary Latent Variables\u003c/b\u003e\u003cbr\u003eID: W2158164339\u003cbr\u003eYear: 2007\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.7551/mitpress/7503.003.0173\u0027\u003ehttps://doi.org/10.7551/mitpress/7503.003.0173\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W1810424324", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eA Hierarchical Community of Experts\u003c/b\u003e\u003cbr\u003eID: W1810424324\u003cbr\u003eYear: 1998\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1007/978-94-011-5014-9_17\u0027\u003ehttps://doi.org/10.1007/978-94-011-5014-9_17\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W1499991161", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eMetric Learning for Large Scale Image Classification: Generalizing to New Classes at Near-Zero Cost\u003c/b\u003e\u003cbr\u003eID: W1499991161\u003cbr\u003eYear: 2012\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1007/978-3-642-33709-3_35\u0027\u003ehttps://doi.org/10.1007/978-3-642-33709-3_35\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2100002341", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eReplicated Softmax: an Undirected Topic Model\u003c/b\u003e\u003cbr\u003eID: W2100002341\u003cbr\u003eYear: 2009\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027N/A\u0027\u003eN/A\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W1576445103", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eCaltech-256 Object Category Dataset\u003c/b\u003e\u003cbr\u003eID: W1576445103\u003cbr\u003eYear: 2007\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027N/A\u0027\u003eN/A\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2116064496", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eTraining Products of Experts by Minimizing Contrastive Divergence\u003c/b\u003e\u003cbr\u003eID: W2116064496\u003cbr\u003eYear: 2002\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1162/089976602760128018\u0027\u003ehttps://doi.org/10.1162/089976602760128018\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2099866409", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eRestricted Boltzmann machines for collaborative filtering\u003c/b\u003e\u003cbr\u003eID: W2099866409\u003cbr\u003eYear: 2007\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1145/1273496.1273596\u0027\u003ehttps://doi.org/10.1145/1273496.1273596\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2613634265", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eScaling learning algorithms towards AI\u003c/b\u003e\u003cbr\u003eID: W2613634265\u003cbr\u003eYear: 2007\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027N/A\u0027\u003eN/A\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2536626143", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eAttribute and simile classifiers for face verification\u003c/b\u003e\u003cbr\u003eID: W2536626143\u003cbr\u003eYear: 2009\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1109/iccv.2009.5459250\u0027\u003ehttps://doi.org/10.1109/iccv.2009.5459250\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2546302380", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eWhat is the best multi-stage architecture for object recognition?\u003c/b\u003e\u003cbr\u003eID: W2546302380\u003cbr\u003eYear: 2009\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1109/iccv.2009.5459469\u0027\u003ehttps://doi.org/10.1109/iccv.2009.5459469\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2165225968", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eUnsupervised learning of distributions on binary vectors using two layer networks\u003c/b\u003e\u003cbr\u003eID: W2165225968\u003cbr\u003eYear: 1991\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027N/A\u0027\u003eN/A\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W1994530392", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eAdvances in neural information processing systems 1\u003c/b\u003e\u003cbr\u003eID: W1994530392\u003cbr\u003eYear: 1989\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027N/A\u0027\u003eN/A\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W1994197834", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eAn empirical evaluation of deep architectures on problems with many factors of variation\u003c/b\u003e\u003cbr\u003eID: W1994197834\u003cbr\u003eYear: 2007\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1145/1273496.1273556\u0027\u003ehttps://doi.org/10.1145/1273496.1273556\u003c/a\u003e"}, {"color": "#1f77b4", "font": {"color": "#000000"}, "id": "W2734776202", "label": "Paper W2734776202", "shape": "dot", "size": 25, "title": "\u003cb\u003ePaper W2734776202\u003c/b\u003e\u003cbr\u003eID: W2734776202\u003cbr\u003eYear: Unknown\u003cbr\u003ePath: ground_truth\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027N/A\u0027\u003eN/A\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W2026942141", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eWhy is Real-World Visual Object Recognition Hard?\u003c/b\u003e\u003cbr\u003eID: W2026942141\u003cbr\u003eYear: 2008\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1371/journal.pcbi.0040027\u0027\u003ehttps://doi.org/10.1371/journal.pcbi.0040027\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W1573503290", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eBeyond Regression : \"New Tools for Prediction and Analysis in the Behavioral Sciences\u003c/b\u003e\u003cbr\u003eID: W1573503290\u003cbr\u003eYear: 1974\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027N/A\u0027\u003eN/A\u003c/a\u003e"}, {"color": "#ff7f0e", "font": {"color": "#000000"}, "id": "W1665214252", "label": "Rectified Linear Units Improve Restricted Boltzmann Machines [Failed]", "shape": "dot", "size": 35, "title": "\u003cb\u003eRectified Linear Units Improve Restricted Boltzmann Machines\u003c/b\u003e\u003cbr\u003eID: W1665214252\u003cbr\u003eYear: 2010\u003cbr\u003ePath: agent_path\u003cbr\u003eType: agent_path\u003cbr\u003eDOI: \u003ca href=\u0027N/A\u0027\u003eN/A\u003c/a\u003e"}, {"color": "#999999", "font": {"color": "#000000"}, "id": "W1509928947", "label": "\u2022", "shape": "dot", "size": 12, "title": "\u003cb\u003eSimilarity Scores Based on Background Samples\u003c/b\u003e\u003cbr\u003eID: W1509928947\u003cbr\u003eYear: 2010\u003cbr\u003ePath: referenced_only\u003cbr\u003eType: referenced\u003cbr\u003eDOI: \u003ca href=\u0027https://doi.org/10.1007/978-3-642-12304-7_9\u0027\u003ehttps://doi.org/10.1007/978-3-642-12304-7_9\u003c/a\u003e"}]);
                  edges = new vis.DataSet([{"color": "#cccccc", "from": "W2097356275", "title": "referenced_only", "to": "W2618530766", "width": 1.5}, {"color": "#cccccc", "from": "W2114153178", "title": "referenced_only", "to": "W1665214252", "width": 1.5}, {"color": "#cccccc", "from": "W2100495367", "title": "referenced_only", "to": "W1665214252", "width": 1.5}, {"color": "#cccccc", "from": "W2064082346", "title": "referenced_only", "to": "W1665214252", "width": 1.5}, {"color": "#cccccc", "from": "W2115761837", "title": "referenced_only", "to": "W1665214252", "width": 1.5}, {"color": "#cccccc", "from": "W2108598243", "title": "referenced_only", "to": "W2618530766", "width": 1.5}, {"color": "#cccccc", "from": "W2015861736", "title": "referenced_only", "to": "W2618530766", "width": 1.5}, {"color": "#cccccc", "from": "W2018435387", "title": "referenced_only", "to": "W2618530766", "width": 1.5}, {"color": "#cccccc", "from": "W2157364932", "title": "referenced_only", "to": "W1665214252", "width": 1.5}, {"color": "#cccccc", "from": "W2166049352", "title": "referenced_only", "to": "W2618530766", "width": 1.5}, {"color": "#cccccc", "from": "W2134557905", "title": "referenced_only", "to": "W2618530766", "width": 1.5}, {"color": "#cccccc", "from": "W2134557905", "title": "referenced_only", "to": "W1665214252", "width": 1.5}, {"color": "#1f77b4", "from": "W2618530766", "title": "ground_truth", "to": "W2734776202", "width": 2}, {"color": "#ff7f0e", "from": "W2618530766", "title": "agent_path", "to": "W1665214252", "width": 2}, {"color": "#cccccc", "from": "W2618530766", "title": "referenced_only", "to": "W1499991161", "width": 1.5}, {"color": "#cccccc", "from": "W2618530766", "title": "referenced_only", "to": "W1573503290", "width": 1.5}, {"color": "#cccccc", "from": "W2618530766", "title": "referenced_only", "to": "W1576445103", "width": 1.5}, {"color": "#cccccc", "from": "W2618530766", "title": "referenced_only", "to": "W1904365287", "width": 1.5}, {"color": "#cccccc", "from": "W2618530766", "title": "referenced_only", "to": "W1994530392", "width": 1.5}, {"color": "#cccccc", "from": "W2618530766", "title": "referenced_only", "to": "W2026942141", "width": 1.5}, {"color": "#cccccc", "from": "W2618530766", "title": "referenced_only", "to": "W2053229256", "width": 1.5}, {"color": "#cccccc", "from": "W2618530766", "title": "referenced_only", "to": "W2061212083", "width": 1.5}, {"color": "#cccccc", "from": "W2618530766", "title": "referenced_only", "to": "W2097117768", "width": 1.5}, {"color": "#cccccc", "from": "W2618530766", "title": "referenced_only", "to": "W2101926813", "width": 1.5}, {"color": "#cccccc", "from": "W2618530766", "title": "referenced_only", "to": "W2108069432", "width": 1.5}, {"color": "#cccccc", "from": "W2618530766", "title": "referenced_only", "to": "W2110764733", "width": 1.5}, {"color": "#cccccc", "from": "W2618530766", "title": "referenced_only", "to": "W2130325614", "width": 1.5}, {"color": "#cccccc", "from": "W2618530766", "title": "referenced_only", "to": "W2141125852", "width": 1.5}, {"color": "#cccccc", "from": "W2618530766", "title": "referenced_only", "to": "W2144161366", "width": 1.5}, {"color": "#cccccc", "from": "W2618530766", "title": "referenced_only", "to": "W2154579312", "width": 1.5}, {"color": "#cccccc", "from": "W2618530766", "title": "referenced_only", "to": "W2156163116", "width": 1.5}, {"color": "#cccccc", "from": "W2618530766", "title": "referenced_only", "to": "W2169805405", "width": 1.5}, {"color": "#cccccc", "from": "W2149845449", "title": "referenced_only", "to": "W1665214252", "width": 1.5}, {"color": "#1f77b4", "from": "W1545791100", "title": "ground_truth", "to": "W2734776202", "width": 2}, {"color": "#1f77b4", "from": "W1545791100", "title": "ground_truth", "to": "W1995875735", "width": 2}, {"color": "#cccccc", "from": "W145818128", "title": "referenced_only", "to": "W1665214252", "width": 1.5}, {"color": "#cccccc", "from": "W2136922672", "title": "referenced_only", "to": "W1665214252", "width": 1.5}, {"color": "#cccccc", "from": "W1782590233", "title": "referenced_only", "to": "W1665214252", "width": 1.5}, {"color": "#cccccc", "from": "W2158164339", "title": "referenced_only", "to": "W1665214252", "width": 1.5}, {"color": "#cccccc", "from": "W1810424324", "title": "referenced_only", "to": "W1665214252", "width": 1.5}, {"color": "#cccccc", "from": "W2100002341", "title": "referenced_only", "to": "W1665214252", "width": 1.5}, {"color": "#cccccc", "from": "W2116064496", "title": "referenced_only", "to": "W1665214252", "width": 1.5}, {"color": "#cccccc", "from": "W2099866409", "title": "referenced_only", "to": "W1665214252", "width": 1.5}, {"color": "#cccccc", "from": "W2613634265", "title": "referenced_only", "to": "W1665214252", "width": 1.5}, {"color": "#cccccc", "from": "W2536626143", "title": "referenced_only", "to": "W1665214252", "width": 1.5}, {"color": "#cccccc", "from": "W2546302380", "title": "referenced_only", "to": "W1665214252", "width": 1.5}, {"color": "#cccccc", "from": "W2165225968", "title": "referenced_only", "to": "W1665214252", "width": 1.5}, {"color": "#cccccc", "from": "W1994197834", "title": "referenced_only", "to": "W1665214252", "width": 1.5}, {"color": "#cccccc", "from": "W1665214252", "title": "referenced_only", "to": "W1509928947", "width": 1.5}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {"physics": {"enabled": true, "stabilization": {"enabled": true, "iterations": 100, "updateInterval": 100}, "barnesHut": {"gravitationalConstant": -2500, "centralGravity": 0.3, "springLength": 200, "springConstant": 0.05, "damping": 0.09, "avoidOverlap": 10}, "minVelocity": 0.75, "maxVelocity": 10, "solver": "barnesHut", "adaptiveTimestep": true}, "interaction": {"dragNodes": true, "dragView": true, "zoomView": true}};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  
                  // make a custom popup
                      var popup = document.createElement("div");
                      popup.className = 'popup';
                      popupTimeout = null;
                      popup.addEventListener('mouseover', function () {
                          console.log(popup)
                          if (popupTimeout !== null) {
                              clearTimeout(popupTimeout);
                              popupTimeout = null;
                          }
                      });
                      popup.addEventListener('mouseout', function () {
                          if (popupTimeout === null) {
                              hidePopup();
                          }
                      });
                      container.appendChild(popup);


                      // use the popup event to show
                      network.on("showPopup", function (params) {
                          showPopup(params);
                      });

                      // use the hide event to hide it
                      network.on("hidePopup", function (params) {
                          hidePopup();
                      });

                      // hiding the popup through css
                      function hidePopup() {
                          popupTimeout = setTimeout(function () { popup.style.display = 'none'; }, 500);
                      }

                      // showing the popup
                      function showPopup(nodeId) {
                          // get the data from the vis.DataSet
                          var nodeData = nodes.get([nodeId]);
                          popup.innerHTML = nodeData[0].title;

                          // get the position of the node
                          var posCanvas = network.getPositions([nodeId])[nodeId];

                          // get the bounding box of the node
                          var boundingBox = network.getBoundingBox(nodeId);

                          //position tooltip:
                          posCanvas.x = posCanvas.x + 0.5 * (boundingBox.right - boundingBox.left);

                          // convert coordinates to the DOM space
                          var posDOM = network.canvasToDOM(posCanvas);

                          // Give it an offset
                          posDOM.x += 10;
                          posDOM.y -= 20;

                          // show and place the tooltip.
                          popup.style.display = 'block';
                          popup.style.top = posDOM.y + 'px';
                          popup.style.left = posDOM.x + 'px';
                      }
                  


                  

                  return network;

              }
              drawGraph();
        </script>
    </body>
</html>